{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe93dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necerssary libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281673b",
   "metadata": {},
   "source": [
    "Decided to scrap the glassdoor job board to get insights into the demand for Power Bi versus Tableau in the global analytics job market. The scrapper will pretty much work like the myjobmag scrapper, upon isnpection of the website's HTML structure, once you search a job keyword, say 'Business Intelligence Analyst', you get redirected to a results page populated with several job cards. To get access to the full job description of the job, you have to click on the job card, which dynamically loads a window on the right with the full details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda7926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to initialize the browser\n",
    "\n",
    "def init_driver():\n",
    "#set up the browser using the options object\n",
    "\n",
    "    options=webdriver.ChromeOptions()\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_experimental_option('excludeSwitches',['enable_automation'])\n",
    "    options.add_experimental_option('useAutomationExtension','False')\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    ")   \n",
    "    stealth (driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    "        )\n",
    "    return driver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41a027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming... 0 jobs already scraped.\n"
     ]
    }
   ],
   "source": [
    "#adding the resume feature, if a job has already been scrapped, the scrapper will skip it\n",
    "\n",
    "scraped_links = set()\n",
    "\n",
    "if os.path.exists(\"glassdoor_data.csv\"):\n",
    "    with open(\"glassdoor_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            scraped_links.add(row[\"link\"])\n",
    "\n",
    "print(f\"Resuming... {len(scraped_links)} jobs already scraped.\")\n",
    "\n",
    "# append mode, so as not to overide data already in the csv\n",
    "csv_file = open(\"glassdoor_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "writer = csv.DictWriter(csv_file, fieldnames=[\"title\", \"description\", \"link\"])\n",
    "\n",
    "# Write header only if file is empty\n",
    "if os.stat(\"glassdoor_data.csv\").st_size == 0:\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1551d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the driver\n",
    "driver=init_driver()\n",
    "base_url=('https://www.glassdoor.com/Job/jobs.htm?sc.occupationParam=power+bi%2C+tableau&sc.locationSeoString=United+States&locId=1&locT=N')\n",
    "time.sleep(random.uniform(4,6))\n",
    "\n",
    "counter=0\n",
    "job_links=[]\n",
    "for count in range(1,2): #scrap for 50 loops only\n",
    "    driver.get(base_url)\n",
    "    time.sleep(random.uniform(4,6))\n",
    "    link_elements = driver.find_elements(By.CLASS_NAME, \"JobCard_jobTitle\")\n",
    "    for element in link_elements:\n",
    "        href=element.get_attribute('href')\n",
    "        job_links.append(href)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31d7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d8d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
